# Validation Report Template

Use this template when generating validation reports.

## Report Structure

```markdown
# Acceptance Test Report: {PRD_ID}

**PRD**: {prd_title}
**Executed**: {ISO8601_timestamp}
**Duration**: {total_seconds} seconds
**Environment**: {environment_name} ({base_url})
**Triggered By**: {trigger_source} (e.g., "validation-test-agent --mode=e2e --task_id=TASK-123")

---

## Summary

| Status | Count | Percentage |
|--------|-------|------------|
| ✅ PASS | {pass_count} | {pass_pct}% |
| ❌ FAIL | {fail_count} | {fail_pct}% |
| ⏭️ SKIP | {skip_count} | {skip_pct}% |

**Overall Verdict**: {PASS | PARTIAL | FAIL} - {one_line_summary}

---

## Results by Criterion

{for each criterion}

### {status_emoji} {criterion_id} ({PASS|FAIL|SKIP})
**Title**: {criterion_title}
**Duration**: {duration}s
**Evidence**: [{evidence_filename}](./evidence/{evidence_filename})

**Verification**:
{for each step}
- {✓|✗} {step_description}: {step_result}
{end for}

{if FAIL}
**Expected**:
```
{expected_outcome}
```

**Actual**:
```
{actual_outcome}
```

**Failure Analysis**:
- Step {failed_step_id} failed: {failure_description}
- {root_cause_hypothesis}

**Recommended Action**:
{numbered_action_list}
{end if}

---

{end for}

## What Works
{bullet_list_of_passing_functionality}

## What Doesn't Work
{bullet_list_of_failing_functionality}

## Blocking Issues
{for each critical failure}
- [ ] {criterion_id} must pass before task can be closed
{end for}

## Recommendations
{numbered_priority_ordered_recommendations}

---

## Evidence Files
| File | Criterion | Description |
|------|-----------|-------------|
{for each evidence_file}
| {filename} | {criterion_id} | {description} |
{end for}

---

*Report generated by acceptance-test-runner skill*
*Timestamp: {ISO8601_timestamp}*
```

## Status Emojis

| Status | Emoji |
|--------|-------|
| PASS | ✅ |
| FAIL | ❌ |
| SKIP | ⏭️ |

## Overall Verdict Logic

```python
def determine_verdict(results):
    if all(r.status == "PASS" for r in results):
        return "PASS", "All acceptance criteria met"

    critical_failures = [r for r in results if r.status == "FAIL" and r.priority == "critical"]
    if critical_failures:
        return "FAIL", f"{len(critical_failures)} critical criteria failing"

    failures = [r for r in results if r.status == "FAIL"]
    if failures:
        return "PARTIAL", f"Core functionality works, {len(failures)} non-critical criteria failing"

    return "PASS", "All criteria passed or skipped"
```

## Failure Analysis Template

When a test fails, include:

```markdown
**Failure Analysis**:
- Step {step_id} failed: {what_the_step_tried_to_do}
- Expected: {what_should_have_happened}
- Actual: {what_actually_happened}
- Error: {error_message_if_any}

**Root Cause Hypothesis**:
{best_guess_at_why_based_on_symptoms}

**Recommended Action**:
1. {specific_action_to_fix}
2. {additional_verification_step}
3. Re-run this acceptance test
```

## Evidence Linking

Evidence files should be:
- Stored in `acceptance-tests/{PRD_ID}/runs/evidence/`
- Named as `{criterion_id}-{description}.{ext}`
- Linked in report using relative paths: `[filename](./evidence/filename)`

Example evidence table:
```markdown
## Evidence Files
| File | Criterion | Description |
|------|-----------|-------------|
| AC-user-login-success.png | AC-user-login | Dashboard after successful login |
| AC-user-login-form.png | AC-user-login | Initial login form state |
| AC-api-auth-response.json | AC-api-authentication | API response for invalid token |
```

## Recommendations Section

Prioritize recommendations:
1. **Immediate**: Blocking issues that prevent task closure
2. **Required**: Non-blocking but important fixes
3. **Suggested**: Nice-to-have improvements

Example:
```markdown
## Recommendations
1. **Immediate**: Implement password reset endpoint (blocks AC-password-reset-complete)
2. **Required**: Add loading state to login button (improves AC-user-login reliability)
3. **Suggested**: Add rate limiting to login API (security improvement, not in PRD)
```
