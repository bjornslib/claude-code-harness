"""TaskMaster bridge â€” calls task-master-ai CLI via subprocess."""

import json
import logging
import subprocess
import tempfile
from pathlib import Path

logger = logging.getLogger(__name__)


def create_enriched_input(sd_path: str, repomap_context: str) -> str:
    """Create enriched SD + RepoMap context string for TaskMaster input.

    Appends a structured codebase context block to the SD content.
    TaskMaster uses this to generate file-aware task decompositions.

    Args:
        sd_path: Path to the Solution Design markdown file
        repomap_context: YAML string from get_repomap_context(format="yaml")

    Returns:
        Combined string: SD content + codebase context block
    """
    sd_content = Path(sd_path).read_text(encoding="utf-8") if Path(sd_path).exists() else ""

    return f"""{sd_content}

---

## Codebase Context (Auto-Generated by CoBuilder RepoMap)

```yaml
{repomap_context}
```

IMPORTANT for task decomposition:
- EXISTING modules: DO NOT create implementation tasks. Reference only.
- MODIFIED modules: Create scoped modification tasks with exact file paths from context.
- NEW modules: Create full implementation tasks using suggested_structure paths.
- Use dependency_graph to order tasks correctly.
- Use key_interfaces for accurate function signatures in task descriptions.
"""


def run_taskmaster_parse(
    enriched_sd_path: str,
    project_root: str,
    repomap_context: str = "",
) -> dict:
    """Call task-master-ai parse-prd via subprocess.

    Returns parsed tasks dict from .taskmaster/tasks/tasks.json.
    Returns {} on timeout or failure (logged, not raised).

    Args:
        enriched_sd_path: Path to the Solution Design (or pre-enriched) file.
        project_root: Root directory of the target project.
        repomap_context: Optional YAML context string from CoBuilder RepoMap.
            When provided, creates an enriched input file via create_enriched_input()
            and passes that to TaskMaster instead of enriched_sd_path directly.
    """
    input_path = enriched_sd_path
    tmp_file = None

    if repomap_context:
        enriched_content = create_enriched_input(enriched_sd_path, repomap_context)
        tmp_file = tempfile.NamedTemporaryFile(
            mode="w", suffix=".md", delete=False, encoding="utf-8"
        )
        tmp_file.write(enriched_content)
        tmp_file.flush()
        tmp_file.close()
        input_path = tmp_file.name

    try:
        result = subprocess.run(
            ["npx", "task-master-ai", "parse-prd",
             "--input", input_path,
             "--project-root", project_root],
            capture_output=True, text=True, timeout=120
        )
        if result.returncode != 0:
            logger.warning("task-master-ai failed (rc=%d): %s", result.returncode, result.stderr[:500])
            return {}
    except subprocess.TimeoutExpired:
        logger.warning("task-master-ai timed out after 120s")
        return {}
    except FileNotFoundError:
        logger.warning("npx not found â€” skipping TaskMaster parse")
        return {}
    finally:
        if tmp_file is not None:
            Path(tmp_file.name).unlink(missing_ok=True)

    tasks_path = Path(project_root) / ".taskmaster" / "tasks" / "tasks.json"
    if not tasks_path.exists():
        logger.warning("tasks.json not found at %s", tasks_path)
        return {}

    try:
        return json.loads(tasks_path.read_text())
    except json.JSONDecodeError as e:
        logger.warning("Failed to parse tasks.json: %s", e)
        return {}


def extract_task_ids_for_node(tasks: dict, node_title: str) -> list[dict]:
    """Find TaskMaster tasks matching a pipeline node by title similarity.

    Returns list of {id, title, subtasks: []} dicts.
    """
    if not tasks or not node_title:
        return []

    node_words = set(node_title.lower().split())
    matches = []

    task_list = tasks.get("tasks", []) if isinstance(tasks, dict) else []
    for task in task_list:
        task_title = task.get("title", "")
        task_words = set(task_title.lower().split())

        if not node_words or not task_words:
            continue

        overlap = len(node_words & task_words) / max(len(node_words), len(task_words))
        if overlap >= 0.4:  # 40% word overlap threshold
            matches.append({
                "id": task.get("id"),
                "title": task_title,
                "subtasks": [s.get("id") for s in task.get("subtasks", [])],
            })

    return matches
