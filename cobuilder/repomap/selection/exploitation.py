"""Exploitation Retriever â€“ vector search with LLM query augmentation.

Implements Task 2.2.1 of PRD-RPG-P2-001 (Epic 2.2: Explore-Exploit Subtree
Selection). Uses vector similarity search over the feature ontology combined
with LLM-powered query expansion to retrieve the most relevant features for a
given user specification.

The retriever operates in two phases:

1. **Query Augmentation**: An LLM expands the user's specification into multiple
   targeted search queries (keywords, related concepts, synonyms) to improve
   recall beyond a single literal query.

2. **Vector Retrieval**: Each augmented query is searched against the ontology
   store, results are merged, de-duplicated, and re-ranked by aggregate score.

Example::

    from cobuilder.repomap.selection import ExploitationRetriever, ExploitationConfig
    from cobuilder.repomap.ontology import OntologyChromaStore
    from cobuilder.repomap.llm import LLMGateway, ModelTier

    store = OntologyChromaStore()
    store.initialize(project_dir)
    gateway = LLMGateway()

    retriever = ExploitationRetriever(
        store=store,
        llm_gateway=gateway,
    )

    result = retriever.retrieve(
        query="Build a real-time chat app with React and WebSocket",
        top_k=20,
    )
    for path in result.paths:
        print(f"{path.leaf.name}: {path.score:.3f}")
"""

from __future__ import annotations

import json
import logging
from typing import Any, Optional

from pydantic import BaseModel, ConfigDict, Field

from cobuilder.repomap.llm.gateway import LLMGateway
from cobuilder.repomap.llm.models import ModelTier
from cobuilder.repomap.ontology.backend import OntologyBackend
from cobuilder.repomap.ontology.models import FeaturePath

logger = logging.getLogger(__name__)


# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------


class ExploitationConfig(BaseModel):
    """Configuration for the ExploitationRetriever.

    Attributes:
        augmentation_tier: LLM tier used for query augmentation.
        max_augmented_queries: Maximum number of augmented queries to generate.
        top_k_per_query: Number of results to retrieve per query.
        score_decay: Multiplicative decay applied to scores from augmented
            (non-original) queries, ensuring the original query retains
            priority. Value in (0.0, 1.0].
        min_score: Minimum similarity score to include in results.
        enable_augmentation: Whether to use LLM query augmentation. When
            False, only the raw query is searched.
    """

    model_config = ConfigDict(frozen=False, validate_assignment=True)

    augmentation_tier: ModelTier = Field(
        default=ModelTier.CHEAP,
        description="LLM tier for query augmentation",
    )
    max_augmented_queries: int = Field(
        default=5,
        ge=1,
        le=20,
        description="Maximum number of augmented queries from LLM",
    )
    top_k_per_query: int = Field(
        default=10,
        ge=1,
        le=100,
        description="Results to retrieve per individual query",
    )
    score_decay: float = Field(
        default=0.85,
        gt=0.0,
        le=1.0,
        description="Score decay for augmented query results",
    )
    min_score: float = Field(
        default=0.0,
        ge=0.0,
        le=1.0,
        description="Minimum score threshold for included results",
    )
    enable_augmentation: bool = Field(
        default=True,
        description="Enable LLM-based query augmentation",
    )


# ---------------------------------------------------------------------------
# LLM response schema for query augmentation
# ---------------------------------------------------------------------------


class _AugmentedQueries(BaseModel):
    """Schema for LLM-generated augmented queries."""

    model_config = ConfigDict(frozen=True)

    queries: list[str] = Field(
        ...,
        description="List of augmented search queries",
    )


# ---------------------------------------------------------------------------
# Retrieval result
# ---------------------------------------------------------------------------


class RetrievalResult(BaseModel):
    """Result of an exploitation retrieval operation.

    Contains the merged, de-duplicated, and re-ranked feature paths from
    all queries, along with metadata about the retrieval process.

    Attributes:
        paths: Ordered list of FeaturePath results, highest score first.
        original_query: The user's original input query.
        augmented_queries: The LLM-generated augmented queries (if any).
        total_candidates: Total number of candidate results before merging.
        retrieval_metadata: Additional metadata about the retrieval.
    """

    model_config = ConfigDict(frozen=True)

    paths: list[FeaturePath] = Field(
        default_factory=list,
        description="Ordered feature paths, best match first",
    )
    original_query: str = Field(
        ...,
        description="The original search query",
    )
    augmented_queries: list[str] = Field(
        default_factory=list,
        description="LLM-generated augmented queries used",
    )
    total_candidates: int = Field(
        default=0,
        ge=0,
        description="Total candidate results before de-duplication",
    )
    retrieval_metadata: dict[str, Any] = Field(
        default_factory=dict,
        description="Additional retrieval process metadata",
    )

    @property
    def top_result(self) -> FeaturePath | None:
        """Return the highest-scoring result, or None if empty."""
        return self.paths[0] if self.paths else None

    @property
    def count(self) -> int:
        """Return the number of results."""
        return len(self.paths)


# ---------------------------------------------------------------------------
# Exploitation Retriever
# ---------------------------------------------------------------------------


_AUGMENTATION_PROMPT = """\
You are a feature search query expander for a software feature ontology.

Given a user's repository specification or description, generate {n} focused \
search queries that would help find relevant features in a hierarchical \
feature ontology. Each query should target a different aspect of the \
specification.

Guidelines:
- Extract key technical terms (languages, frameworks, protocols, patterns)
- Include both specific terms and broader category terms
- Add related synonyms and alternative names
- Keep each query concise (2-6 words)
- Cover different facets: core functionality, infrastructure, tooling, \
  quality attributes

User specification:
{query}

Respond with a JSON object containing a "queries" field with an array of \
{n} string queries. Example:
{{"queries": ["react frontend components", "websocket real-time messaging", \
"user authentication oauth"]}}
"""


class ExploitationRetriever:
    """Vector search retriever with LLM-powered query augmentation.

    Wraps an :class:`~zerorepo.ontology.backend.OntologyBackend` to provide
    enhanced feature retrieval. The retriever first uses an LLM to expand
    the user's query into multiple targeted search terms, then queries the
    ontology store with each, and finally merges and re-ranks results.

    Args:
        store: An initialized OntologyBackend (typically OntologyChromaStore).
        llm_gateway: An LLMGateway instance for query augmentation.
        config: Optional retriever configuration.

    Example::

        retriever = ExploitationRetriever(store=store, llm_gateway=gw)
        result = retriever.retrieve("real-time chat with React", top_k=15)
    """

    def __init__(
        self,
        store: OntologyBackend,
        llm_gateway: LLMGateway | None = None,
        config: ExploitationConfig | None = None,
    ) -> None:
        self._store = store
        self._llm = llm_gateway
        self._config = config or ExploitationConfig()

    @property
    def store(self) -> OntologyBackend:
        """Return the underlying ontology backend."""
        return self._store

    @property
    def config(self) -> ExploitationConfig:
        """Return the retriever configuration."""
        return self._config

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------

    def retrieve(
        self,
        query: str,
        top_k: int = 20,
        level: int | None = None,
        tags: list[str] | None = None,
    ) -> RetrievalResult:
        """Retrieve relevant features for a query.

        Performs LLM-augmented search when enabled and an LLM gateway is
        available, otherwise falls back to direct vector search.

        Args:
            query: Natural language query or specification excerpt.
            top_k: Maximum number of results to return.
            level: Optional filter by ontology level.
            tags: Optional filter by tags.

        Returns:
            A :class:`RetrievalResult` with ranked feature paths.

        Raises:
            ValueError: If query is empty or top_k is not positive.
        """
        if not query or not query.strip():
            raise ValueError("query must not be empty")
        if top_k <= 0:
            raise ValueError("top_k must be positive")

        query = query.strip()

        # Phase 1: Query augmentation
        augmented_queries = self._augment_query(query)

        # Phase 2: Multi-query retrieval
        all_queries = [query] + augmented_queries
        all_paths, total_candidates = self._multi_query_search(
            queries=all_queries,
            level=level,
            tags=tags,
        )

        # Phase 3: Merge, de-duplicate, re-rank
        merged = self._merge_and_rank(all_paths, top_k)

        logger.info(
            "ExploitationRetriever: query=%r, augmented=%d, "
            "candidates=%d, returned=%d",
            query[:60],
            len(augmented_queries),
            total_candidates,
            len(merged),
        )

        return RetrievalResult(
            paths=merged,
            original_query=query,
            augmented_queries=augmented_queries,
            total_candidates=total_candidates,
            retrieval_metadata={
                "top_k": top_k,
                "queries_used": len(all_queries),
                "augmentation_enabled": (
                    self._config.enable_augmentation and self._llm is not None
                ),
                "level_filter": level,
                "tag_filter": tags,
            },
        )

    def retrieve_for_spec(
        self,
        description: str,
        languages: list[str] | None = None,
        frameworks: list[str] | None = None,
        core_functionality: str | None = None,
        top_k: int = 20,
    ) -> RetrievalResult:
        """Retrieve features for a structured specification.

        Constructs an enriched query from specification fields for more
        targeted retrieval.

        Args:
            description: The specification description text.
            languages: Target programming languages.
            frameworks: Framework preferences.
            core_functionality: Core functionality summary.
            top_k: Maximum number of results.

        Returns:
            A :class:`RetrievalResult` with ranked feature paths.

        Raises:
            ValueError: If description is empty.
        """
        if not description or not description.strip():
            raise ValueError("description must not be empty")

        # Build enriched query from spec fields
        parts = [description.strip()]
        if core_functionality:
            parts.append(f"Core: {core_functionality.strip()}")
        if languages:
            parts.append(f"Languages: {', '.join(languages)}")
        if frameworks:
            parts.append(f"Frameworks: {', '.join(frameworks)}")

        enriched_query = " | ".join(parts)
        return self.retrieve(query=enriched_query, top_k=top_k)

    # ------------------------------------------------------------------
    # Internal: Query augmentation
    # ------------------------------------------------------------------

    def _augment_query(self, query: str) -> list[str]:
        """Generate augmented search queries via LLM.

        Falls back to an empty list if augmentation is disabled, no LLM
        is available, or the LLM call fails.

        Args:
            query: The original search query.

        Returns:
            List of augmented query strings (may be empty).
        """
        if not self._config.enable_augmentation:
            return []
        if self._llm is None:
            logger.debug("No LLM gateway available; skipping augmentation")
            return []

        n = self._config.max_augmented_queries
        prompt = _AUGMENTATION_PROMPT.format(query=query, n=n)

        try:
            model = self._llm.select_model(self._config.augmentation_tier)
            response_text = self._llm.complete(
                messages=[{"role": "user", "content": prompt}],
                model=model,
                tier=self._config.augmentation_tier,
            )

            # Parse the response
            augmented = self._parse_augmented_response(response_text, n)

            logger.debug(
                "Query augmentation produced %d queries for %r",
                len(augmented),
                query[:40],
            )
            return augmented

        except Exception as exc:
            logger.warning(
                "Query augmentation failed (falling back to original query): %s",
                exc,
            )
            return []

    def _parse_augmented_response(
        self,
        response_text: str,
        max_queries: int,
    ) -> list[str]:
        """Parse the LLM response into a list of query strings.

        Handles both structured JSON and unstructured text responses.

        Args:
            response_text: Raw LLM response text.
            max_queries: Maximum number of queries to extract.

        Returns:
            List of query strings (cleaned and truncated).
        """
        # Try JSON parsing first
        try:
            data = json.loads(response_text)
            if isinstance(data, dict) and "queries" in data:
                queries = data["queries"]
                if isinstance(queries, list):
                    cleaned = [
                        str(q).strip()
                        for q in queries
                        if isinstance(q, str) and q.strip()
                    ]
                    return cleaned[:max_queries]
        except (json.JSONDecodeError, TypeError):
            pass

        # Fallback: try to extract from JSON embedded in text
        try:
            start = response_text.index("{")
            end = response_text.rindex("}") + 1
            data = json.loads(response_text[start:end])
            if isinstance(data, dict) and "queries" in data:
                queries = data["queries"]
                if isinstance(queries, list):
                    cleaned = [
                        str(q).strip()
                        for q in queries
                        if isinstance(q, str) and q.strip()
                    ]
                    return cleaned[:max_queries]
        except (ValueError, json.JSONDecodeError, TypeError):
            pass

        # Last resort: split by newlines and use non-empty lines
        lines = [
            line.strip().lstrip("- ").lstrip("* ").strip('"').strip("'")
            for line in response_text.strip().split("\n")
            if line.strip()
        ]
        cleaned = [line for line in lines if 2 <= len(line) <= 200]
        return cleaned[:max_queries]

    # ------------------------------------------------------------------
    # Internal: Multi-query search
    # ------------------------------------------------------------------

    def _multi_query_search(
        self,
        queries: list[str],
        level: int | None = None,
        tags: list[str] | None = None,
    ) -> tuple[list[tuple[FeaturePath, float]], int]:
        """Execute search for each query and collect weighted results.

        Args:
            queries: List of queries. Index 0 is the original query;
                subsequent ones are augmented queries with score decay.
            level: Optional level filter.
            tags: Optional tag filter.

        Returns:
            A tuple of (list of (path, weighted_score), total_candidates).
        """
        all_results: list[tuple[FeaturePath, float]] = []
        total_candidates = 0
        k = self._config.top_k_per_query

        for i, q in enumerate(queries):
            try:
                # Apply score decay for augmented queries (index > 0)
                weight = 1.0 if i == 0 else self._config.score_decay

                # Use filtered search if filters are provided
                if level is not None or tags:
                    paths = self._store.search_with_filters(
                        query=q,
                        top_k=k,
                        level=level,
                        tags=tags,
                    )
                else:
                    paths = self._store.search(query=q, top_k=k)

                total_candidates += len(paths)

                for path in paths:
                    weighted_score = path.score * weight
                    all_results.append((path, weighted_score))

            except Exception as exc:
                logger.warning(
                    "Search failed for query %r: %s",
                    q[:40],
                    exc,
                )

        return all_results, total_candidates

    # ------------------------------------------------------------------
    # Internal: Merge and rank
    # ------------------------------------------------------------------

    def _merge_and_rank(
        self,
        results: list[tuple[FeaturePath, float]],
        top_k: int,
    ) -> list[FeaturePath]:
        """Merge, de-duplicate, and re-rank results from multiple queries.

        De-duplication is based on the leaf node ID. When the same feature
        appears from multiple queries, the highest weighted score is kept.

        Args:
            results: List of (FeaturePath, weighted_score) tuples.
            top_k: Maximum number of results to return.

        Returns:
            Sorted list of FeaturePath objects (highest score first).
        """
        # De-duplicate by leaf node ID, keeping the highest score
        best: dict[str, tuple[FeaturePath, float]] = {}

        for path, score in results:
            leaf_id = path.leaf.id
            if leaf_id not in best or score > best[leaf_id][1]:
                best[leaf_id] = (path, score)

        # Filter by minimum score
        min_score = self._config.min_score
        candidates = [
            (path, score)
            for path, score in best.values()
            if score >= min_score
        ]

        # Sort by score descending
        candidates.sort(key=lambda x: x[1], reverse=True)

        # Apply top_k and rebuild FeaturePath with updated score
        final: list[FeaturePath] = []
        for path, score in candidates[:top_k]:
            # Create a new FeaturePath with the weighted score
            updated = FeaturePath(nodes=path.nodes, score=score)
            final.append(updated)

        return final
