# F4.2 Design Spec: cs-verify Programmatic Gate (Anthropic SDK)

**PRD**: PRD-S3-AUTONOMY-001, Epic 4, Feature F4.2
**Status**: Design Spec (ready for implementation by backend-solutions-engineer)
**Author**: worker-backend
**Date**: 2026-02-17

---

## 1. Overview

Transform `cs-verify` from a bash-only promise-checking tool into a **triple-gate validation system** where Gate 3 calls the Anthropic Messages API with Sonnet 4.5 as a programmatic judge. The LLM evaluates whether the claimed evidence actually satisfies the acceptance criteria, providing an independent AI-powered verification layer.

### Triple-Gate Architecture

```
Gate 1: Session Self-Report
  └─ cs-promise --meet → marks acceptance criteria as met
  └─ cs-verify --check → verifies all criteria marked met
  └─ Current bash logic (UNCHANGED)

Gate 2: s3-validator Teammate (F4.1)
  └─ On-demand teammate independently verifies implementation
  └─ Reports via SendMessage with structured JSON verdict
  └─ Documented in validation-workflow.md

Gate 3: cs-verify --judge (NEW — this spec)
  └─ Calls Anthropic Messages API with Sonnet 4.5
  └─ Input: promise summary, acceptance criteria, proof provided
  └─ Output: JSON verdict (PASS/FAIL), reasoning, confidence
  └─ If FAIL: verification rejected, session cannot end
  └─ Fallback: if API unavailable, warn but allow (graceful degradation)
```

---

## 2. Current State

### Existing cs-verify (`scripts/completion-state/cs-verify`)
- **Language**: Bash (~428 lines)
- **Location**: `.claude/scripts/completion-state/cs-verify`
- **Dependencies**: `jq`, `python3` (for timestamps only)
- **Modes**: `--promise <id>` (verify), `--check` (stop gate)
- **Logic**: Checks if all acceptance criteria have `status: "met"` in JSON files
- **Limitation**: Trusts self-reported evidence — no independent verification of quality

### Existing Verdict Schema (`validation/verdict-schema.json`)
- Defines `ValidationVerdict` with `task_id`, `mode`, `success`, `validation_levels`
- Used by validation-test-agent for structured output
- Gate 3 output should be compatible but distinct (it judges evidence quality, not runs tests)

---

## 3. Proposed Design

### 3.1 New Command: `cs-verify --judge`

Add a new mode to the existing `cs-verify` bash script that invokes a Python companion script for the Anthropic API call.

```bash
# New usage pattern
cs-verify --judge --promise <id>                    # Judge with Sonnet 4.5
cs-verify --judge --promise <id> --model sonnet     # Explicit model (default)
cs-verify --judge --promise <id> --dry-run          # Show what would be sent (no API call)
```

### 3.2 Architecture

```
cs-verify (bash)
    │
    ├── --check mode    → existing logic (unchanged)
    ├── --promise mode  → existing logic (unchanged)
    └── --judge mode    → NEW
            │
            ├── 1. Read promise JSON from completion-state/promises/
            ├── 2. Extract: summary, acceptance_criteria, evidence per AC
            ├── 3. Call Python companion: cs-verify-judge.py
            │       │
            │       ├── Build Anthropic API request
            │       ├── Call claude-sonnet-4-5-20250929
            │       ├── Parse structured JSON response
            │       ├── Log token usage
            │       └── Return verdict JSON
            │
            ├── 4. Parse Python output
            ├── 5. If PASS: proceed to verification
            ├── 6. If FAIL: block with reasoning
            └── 7. If API_ERROR: warn + allow (graceful degradation)
```

### 3.3 Python Companion Script: `cs-verify-judge.py`

**Location**: `.claude/scripts/completion-state/cs-verify-judge.py`
**Dependencies**: `anthropic` Python package (already available in environment)

#### Input (stdin JSON)

```json
{
  "promise_id": "promise-uuid-123",
  "promise_summary": "Complete Epic 4 validation teammate integration",
  "acceptance_criteria": [
    {
      "id": "AC-1",
      "description": "System 3 can spawn s3-validator on-demand",
      "status": "met",
      "evidence": "Updated validation-workflow.md with on-demand spawn pattern. See lines 139-302.",
      "evidence_type": "test",
      "met_at": "2026-02-17T10:30:00Z"
    },
    {
      "id": "AC-2",
      "description": "Validator reports results via SendMessage",
      "status": "met",
      "evidence": "Documented SendMessage pattern in oversight-team.md. Workers report JSON verdict.",
      "evidence_type": "manual",
      "met_at": "2026-02-17T10:45:00Z"
    }
  ],
  "session_id": "orch-epic4",
  "model": "claude-sonnet-4-5-20250929"
}
```

#### Anthropic API Call

```python
import anthropic
import json
import sys
import time

def judge_promise(input_data: dict) -> dict:
    """Call Sonnet 4.5 to judge whether evidence satisfies acceptance criteria."""

    client = anthropic.Anthropic()  # Uses ANTHROPIC_API_KEY from environment

    model = input_data.get("model", "claude-sonnet-4-5-20250929")

    system_prompt = """You are a strict validation judge for a software development project.
Your job is to evaluate whether the provided evidence ACTUALLY satisfies each acceptance criterion.

Rules:
- Evidence must be SPECIFIC and VERIFIABLE (not vague claims like "it works")
- "Manual" evidence type requires detailed description of what was verified
- "Test" evidence type should reference specific test results with pass/fail counts
- "Browser" evidence should reference screenshots or specific UI states observed
- "API" evidence should include HTTP status codes and response content

You MUST output valid JSON matching the schema below. No other text."""

    user_prompt = f"""## Promise to Validate
**ID**: {input_data["promise_id"]}
**Summary**: {input_data["promise_summary"]}

## Acceptance Criteria and Evidence

"""
    for ac in input_data["acceptance_criteria"]:
        user_prompt += f"""### {ac["id"]}: {ac["description"]}
- **Status**: {ac["status"]}
- **Evidence Type**: {ac.get("evidence_type", "unknown")}
- **Evidence**: {ac.get("evidence", "No evidence provided")}
- **Met At**: {ac.get("met_at", "unknown")}

"""

    user_prompt += """## Your Task
Evaluate EACH acceptance criterion independently. For each:
1. Does the evidence actually demonstrate the criterion is met?
2. Is the evidence specific enough to be verifiable?
3. Rate your confidence (0.0-1.0)

Then provide an overall verdict.

## Required Output (JSON only)
```json
{
  "verdict": "PASS" or "FAIL",
  "overall_confidence": 0.0-1.0,
  "reasoning": "Overall assessment in 1-2 sentences",
  "criteria_judgments": [
    {
      "ac_id": "AC-1",
      "judgment": "PASS" or "FAIL",
      "confidence": 0.0-1.0,
      "reasoning": "Why this criterion passes/fails"
    }
  ],
  "token_cost": {
    "input_tokens": 0,
    "output_tokens": 0
  }
}
```"""

    start_time = time.time()

    try:
        response = client.messages.create(
            model=model,
            max_tokens=2048,
            system=system_prompt,
            messages=[{"role": "user", "content": user_prompt}]
        )

        elapsed = time.time() - start_time

        # Extract JSON from response
        response_text = response.content[0].text

        # Try to parse JSON (handle markdown code blocks)
        if "```json" in response_text:
            json_str = response_text.split("```json")[1].split("```")[0].strip()
        elif "```" in response_text:
            json_str = response_text.split("```")[1].split("```")[0].strip()
        else:
            json_str = response_text.strip()

        result = json.loads(json_str)

        # Add metadata
        result["token_cost"] = {
            "input_tokens": response.usage.input_tokens,
            "output_tokens": response.usage.output_tokens
        }
        result["model"] = model
        result["latency_ms"] = int(elapsed * 1000)
        result["status"] = "success"

        return result

    except anthropic.APIError as e:
        return {
            "status": "api_error",
            "error": str(e),
            "verdict": "WARN",
            "reasoning": f"Anthropic API unavailable: {e}. Graceful degradation — allowing verification to proceed.",
            "overall_confidence": 0.0,
            "criteria_judgments": [],
            "token_cost": {"input_tokens": 0, "output_tokens": 0}
        }
    except json.JSONDecodeError as e:
        return {
            "status": "parse_error",
            "error": f"Failed to parse LLM response as JSON: {e}",
            "raw_response": response_text[:500] if 'response_text' in dir() else "no response",
            "verdict": "WARN",
            "reasoning": "Could not parse judge response. Graceful degradation — allowing verification to proceed.",
            "overall_confidence": 0.0,
            "criteria_judgments": [],
            "token_cost": {"input_tokens": 0, "output_tokens": 0}
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e),
            "verdict": "WARN",
            "reasoning": f"Unexpected error: {e}. Graceful degradation — allowing verification to proceed.",
            "overall_confidence": 0.0,
            "criteria_judgments": [],
            "token_cost": {"input_tokens": 0, "output_tokens": 0}
        }


if __name__ == "__main__":
    input_data = json.load(sys.stdin)
    result = judge_promise(input_data)
    json.dump(result, sys.stdout, indent=2)
    sys.exit(0 if result.get("verdict") == "PASS" else
             0 if result.get("verdict") == "WARN" else 1)
```

#### Output (stdout JSON)

**PASS Case**:
```json
{
  "status": "success",
  "verdict": "PASS",
  "overall_confidence": 0.85,
  "reasoning": "All acceptance criteria have specific, verifiable evidence. Documentation changes are referenced by file and line numbers.",
  "criteria_judgments": [
    {
      "ac_id": "AC-1",
      "judgment": "PASS",
      "confidence": 0.9,
      "reasoning": "Evidence references specific file (validation-workflow.md) with line numbers for the on-demand spawn pattern."
    },
    {
      "ac_id": "AC-2",
      "judgment": "PASS",
      "confidence": 0.8,
      "reasoning": "SendMessage pattern is documented in oversight-team.md. JSON verdict format is specified."
    }
  ],
  "token_cost": {
    "input_tokens": 1250,
    "output_tokens": 380
  },
  "model": "claude-sonnet-4-5-20250929",
  "latency_ms": 2340
}
```

**FAIL Case**:
```json
{
  "status": "success",
  "verdict": "FAIL",
  "overall_confidence": 0.75,
  "reasoning": "AC-2 evidence is vague — 'it works' is not verifiable. Specific test results or API responses required.",
  "criteria_judgments": [
    {
      "ac_id": "AC-1",
      "judgment": "PASS",
      "confidence": 0.9,
      "reasoning": "Specific file and line references provided."
    },
    {
      "ac_id": "AC-2",
      "judgment": "FAIL",
      "confidence": 0.8,
      "reasoning": "Evidence states 'manually verified it works' without specifics. Need HTTP status codes, response payloads, or screenshot references."
    }
  ],
  "token_cost": {
    "input_tokens": 1250,
    "output_tokens": 420
  },
  "model": "claude-sonnet-4-5-20250929",
  "latency_ms": 2870
}
```

**API Error (Graceful Degradation)**:
```json
{
  "status": "api_error",
  "verdict": "WARN",
  "error": "Connection refused",
  "reasoning": "Anthropic API unavailable: Connection refused. Graceful degradation — allowing verification to proceed.",
  "overall_confidence": 0.0,
  "criteria_judgments": [],
  "token_cost": {"input_tokens": 0, "output_tokens": 0}
}
```

### 3.4 Bash Integration in cs-verify

Add the `--judge` mode to the existing argument parser and flow:

```bash
# New argument: --judge
--judge)
    JUDGE_MODE=true
    shift
    ;;

# After existing acceptance criteria check (line ~362), add judge gate:
if [ "$JUDGE_MODE" = true ]; then
    # Build input JSON for Python judge
    JUDGE_INPUT=$(jq '{
        promise_id: .id,
        promise_summary: .summary,
        acceptance_criteria: .acceptance_criteria,
        session_id: env.CLAUDE_SESSION_ID,
        model: ($model // "claude-sonnet-4-5-20250929")
    }' --arg model "$JUDGE_MODEL" "$PROMISE_FILE")

    # Call Python judge
    JUDGE_RESULT=$(echo "$JUDGE_INPUT" | python3 "$SCRIPT_DIR/cs-verify-judge.py" 2>/dev/null)
    JUDGE_EXIT=$?

    # Parse result
    JUDGE_VERDICT=$(echo "$JUDGE_RESULT" | jq -r '.verdict')
    JUDGE_REASONING=$(echo "$JUDGE_RESULT" | jq -r '.reasoning')
    JUDGE_CONFIDENCE=$(echo "$JUDGE_RESULT" | jq -r '.overall_confidence')
    JUDGE_STATUS=$(echo "$JUDGE_RESULT" | jq -r '.status')

    # Log token usage
    INPUT_TOKENS=$(echo "$JUDGE_RESULT" | jq -r '.token_cost.input_tokens')
    OUTPUT_TOKENS=$(echo "$JUDGE_RESULT" | jq -r '.token_cost.output_tokens')
    LATENCY=$(echo "$JUDGE_RESULT" | jq -r '.latency_ms')

    echo ""
    echo "=== GATE 3: Programmatic Judge ==="
    echo "Model: $(echo "$JUDGE_RESULT" | jq -r '.model // "unknown"')"
    echo "Verdict: $JUDGE_VERDICT"
    echo "Confidence: $JUDGE_CONFIDENCE"
    echo "Reasoning: $JUDGE_REASONING"
    echo "Tokens: $INPUT_TOKENS in / $OUTPUT_TOKENS out"
    echo "Latency: ${LATENCY}ms"

    # Append to cost log
    COST_LOG="$STATE_DIR/judge-cost-log.jsonl"
    echo "$JUDGE_RESULT" | jq -c '{
        timestamp: (now | todate),
        promise_id: .promise_id,
        input_tokens: .token_cost.input_tokens,
        output_tokens: .token_cost.output_tokens,
        model: .model,
        latency_ms: .latency_ms,
        verdict: .verdict
    }' >> "$COST_LOG" 2>/dev/null

    case "$JUDGE_VERDICT" in
        PASS)
            echo "Gate 3: PASSED"
            # Continue to existing verification logic
            ;;
        FAIL)
            echo ""
            echo "GATE 3 FAILED - Verification rejected by programmatic judge."
            echo ""
            echo "Failed criteria:"
            echo "$JUDGE_RESULT" | jq -r '.criteria_judgments[] | select(.judgment == "FAIL") | "  [\(.ac_id)] \(.reasoning)"'
            echo ""
            echo "Fix the evidence and re-submit before verification can succeed."
            exit 1
            ;;
        WARN)
            echo ""
            echo "WARNING: Gate 3 unavailable (API error). Proceeding with graceful degradation."
            echo "Reason: $JUDGE_REASONING"
            # Continue to existing verification logic (don't block)
            ;;
    esac
fi
```

### 3.5 Cost Tracking

Token usage is logged to `.claude/completion-state/judge-cost-log.jsonl`:

```jsonl
{"timestamp":"2026-02-17T10:30:00Z","promise_id":"promise-123","input_tokens":1250,"output_tokens":380,"model":"claude-sonnet-4-5-20250929","latency_ms":2340,"verdict":"PASS"}
{"timestamp":"2026-02-17T11:15:00Z","promise_id":"promise-456","input_tokens":2100,"output_tokens":520,"model":"claude-sonnet-4-5-20250929","latency_ms":3100,"verdict":"FAIL"}
```

**Estimated cost per judgment**:
- Input: ~1000-3000 tokens (depends on number of ACs and evidence length)
- Output: ~300-600 tokens
- Sonnet 4.5 pricing: ~$3/M input, ~$15/M output
- **Per call: ~$0.003-0.018** (extremely low cost per validation)

### 3.6 Graceful Degradation

The judge is an **enhancement**, not a blocker. If the API is unavailable:

| Scenario | Verdict | Behavior |
|----------|---------|----------|
| API call succeeds, PASS | `PASS` | Verification proceeds normally |
| API call succeeds, FAIL | `FAIL` | Verification blocked with reasoning |
| API connection refused | `WARN` | Warning printed, verification proceeds |
| API rate limited | `WARN` | Warning printed, verification proceeds |
| API key missing | `WARN` | Warning printed, verification proceeds |
| JSON parse error | `WARN` | Warning printed, verification proceeds |
| Python not available | `WARN` | Warning printed, verification proceeds |

**Rationale**: Gate 3 adds rigor but should never prevent work from completing due to infrastructure issues. Gates 1 and 2 already provide baseline validation.

---

## 4. Integration Points

### 4.1 Stop Gate Integration

The existing stop gate calls `cs-verify --check`. Gate 3 is NOT invoked during `--check` mode — it's only invoked during explicit `--judge` mode. This keeps the stop gate fast.

**System 3 workflow**:
```
1. Session self-reports completion     → cs-verify --check (Gate 1)
2. s3-validator verifies independently → SendMessage verdict (Gate 2)
3. System 3 runs final judge          → cs-verify --judge --promise <id> (Gate 3)
4. If all gates pass                   → cs-verify --promise <id> (move to history)
```

### 4.2 Environment Requirements

| Variable | Required | Purpose |
|----------|----------|---------|
| `ANTHROPIC_API_KEY` | Yes (for judge) | Anthropic API authentication |
| `CLAUDE_SESSION_ID` | Yes | Session identification |
| `CLAUDE_PROJECT_DIR` | Optional | Project root (defaults to `pwd`) |

### 4.3 File Locations

| File | Purpose |
|------|---------|
| `.claude/scripts/completion-state/cs-verify` | Main bash script (extend) |
| `.claude/scripts/completion-state/cs-verify-judge.py` | Python companion (new) |
| `.claude/completion-state/judge-cost-log.jsonl` | Token usage log (new) |
| `.claude/validation/verdict-schema.json` | Existing schema (reference) |

---

## 5. Acceptance Criteria

- [ ] `cs-verify --judge --promise <id>` calls Anthropic Messages API with Sonnet 4.5 model
- [ ] Input includes: promise summary, acceptance criteria list, evidence per criterion
- [ ] Output is JSON with: verdict (PASS/FAIL/WARN), reasoning, confidence score, per-criterion judgments
- [ ] If verdict=FAIL: verification is rejected, session cannot end via this promise
- [ ] Cost tracking: token usage logged to `judge-cost-log.jsonl` per validation call
- [ ] Fallback: if API unavailable, print warning but don't block (graceful degradation)
- [ ] `--dry-run` flag shows the prompt that would be sent without calling the API
- [ ] Per-criterion judgment with individual PASS/FAIL and reasoning

---

## 6. Implementation Notes for Backend Engineer

### Dependencies
- `anthropic` Python package (already available — used by other MCP servers)
- `jq` (already available — used throughout cs-* scripts)
- No new pip installs required

### Testing Strategy
1. **Unit test**: Mock the `anthropic.Anthropic` client, verify prompt construction and response parsing
2. **Integration test**: Call real API with a simple test promise (2 ACs, known evidence)
3. **Degradation test**: Simulate API errors (connection refused, rate limit), verify WARN behavior
4. **Cost test**: Verify token logging works correctly

### Edge Cases
- Promise with 0 acceptance criteria (legacy) → skip judge, use existing `--proof` logic
- Promise with > 20 acceptance criteria → may exceed context, truncate evidence to 200 chars each
- Evidence contains code blocks → ensure JSON serialization handles special characters
- Multiple concurrent judge calls → each writes independently to JSONL (append-safe)
- Empty evidence field → judge should flag as FAIL for that criterion

### Security
- API key read from environment only (never logged or included in output)
- Promise data stays local (sent to Anthropic API for judging, but no PII expected)
- Judge output logged to local JSONL only (no external reporting)

---

## 7. Design Decisions

### D1: Python companion script (not rewrite cs-verify in Python)

**Rationale**: cs-verify is 428 lines of working bash. Rewriting introduces risk. The Python companion handles only the API call, keeping the bash script's existing logic intact.

### D2: Sonnet 4.5 (not Opus or Haiku)

**Rationale**:
- Haiku: Too cheap may miss nuance in evidence evaluation
- Sonnet: Good balance of quality and cost (~$0.01 per judgment)
- Opus: Overkill for structured evidence checking; 10x the cost

### D3: WARN instead of FAIL for API errors

**Rationale**: Gate 3 is defense-in-depth, not a primary gate. Gates 1 (self-report) and 2 (s3-validator teammate) already catch most issues. Blocking on API unavailability would halt all work.

### D4: JSONL for cost logging (not SQLite)

**Rationale**: Append-only, no schema migration, grep-friendly, works with concurrent writers. SQLite would require connection management and is overkill for a log file.

---

**Version**: 1.0.0
**Ready for implementation**: Yes — backend-solutions-engineer should create `cs-verify-judge.py` and extend `cs-verify` bash script with `--judge` mode.
