---
prd_id: PRD-EXECBDD-001
title: "Executable BDD Acceptance Tests"
product: "Claude Harness Setup"
version: "1.0"
status: draft
created: "2026-02-22"
author: "System 3 Orchestrator"
priority: P1
---

# PRD-EXECBDD-001: Executable BDD Acceptance Tests

## Problem Statement

The current acceptance test infrastructure generates Gherkin `.feature` files per PRD (e.g., `acceptance-tests/PRD-GCHAT-HOOKS-001/scenarios.feature`). These files contain detailed "Evidence to check" annotations and gradient scoring guides (0.0–1.0) that the s3-guardian manually interprets by reading code.

**The gap**: Tests are not runnable. Validation requires a human (or LLM) to read code and manually score each scenario. This creates:
- Subjectivity in scoring (LLM-to-LLM variance)
- No automated regression detection
- s3-orchestrator cannot self-validate before marking `impl_complete`
- s3-guardian must re-read all code from scratch on each validation pass

**The opportunity**: The "Evidence to check" annotations in existing `.feature` files are largely mechanical — file existence, content grep, JSON/YAML structure, git log. These can be automated as pytest-bdd step definitions, converting the Gherkin rubric from a scoring guide into a genuinely executable test suite.

---

## Goals

1. **Executable tests**: `pytest acceptance-tests/PRD-XXX/` runs and returns pass/fail per scenario
2. **PRD traceability**: Every test is tagged `@prd-{id}` and `@epic-{n}` for filtering
3. **Dual execution**: s3-orchestrator runs tests before `impl_complete`; s3-guardian runs tests independently as blind validation
4. **Backward compatible**: Existing LLM-scored `.feature` files continue to work; step definitions are additive
5. **Living documentation**: `.feature` files remain human-readable specs AND executable tests simultaneously

---

## Non-Goals

- Replacing the gradient scoring system (step pass-rate feeds into gradient, doesn't replace it)
- Handling browser-based E2E tests (those remain in `tdd-test-engineer` domain)
- Testing non-deterministic LLM outputs (semantic matching is out of scope for v1)
- Requiring services to be running (all harness tests run from filesystem/subprocess only)

---

## Background: Research Synthesis

### Executable BDD Architecture
Standard BDD uses a two-layer architecture:
1. **Gherkin feature files** (`.feature`) — human-readable specification
2. **Step definitions** (`.py`) — code that executes each Given/When/Then

`pytest-bdd` is the recommended Python framework because it integrates with the existing pytest infrastructure, supports parameterization, and has full fixture ecosystem access.

### What Makes Claude Harness Tests Special
Unlike typical web-app BDD (browser automation), harness acceptance tests validate:
- **File existence and permissions** (`Given the claude-harness-setup repository, When I check for hook X, Then file exists at path Y`)
- **Configuration correctness** (`Then .claude/settings.json has key "matcher" = "AskUserQuestion"`)
- **Content patterns** (`And the hook source contains "deny" in return value`)
- **Script behavior** (`When gchat-send runs with --type task_completion, Then output contains "✅"`)

These are mechanical and automatable. The "Evidence to check" annotations in existing `.feature` files ARE the step definitions, waiting to be coded.

### Hindsight Pattern: Three-Stage Checkpoint
From established patterns: implementation → validation-agent → closure. Executable BDD tests slot into the validation-agent phase, providing deterministic evidence rather than LLM estimation.

---

## Architecture

```
acceptance-tests/
├── lib/                              # [NEW] Shared step definition library
│   ├── conftest.py                   # pytest fixtures: repo_root, settings_json, etc.
│   └── steps/
│       ├── filesystem_steps.py       # File existence, permissions, content
│       ├── config_steps.py           # JSON/YAML validation, key presence
│       ├── grep_steps.py             # Pattern search in files/directories
│       └── subprocess_steps.py       # Script execution, output validation
├── PRD-GCHAT-HOOKS-001/
│   ├── manifest.yaml                 # [EXISTING] Feature weights + thresholds
│   ├── scenarios.feature             # [EXISTING] Gherkin scenarios
│   └── steps/                        # [NEW] Executable step implementations
│       ├── conftest.py               # PRD-local fixtures
│       └── test_prd_gchat.py         # pytest-bdd step definitions
├── PRD-XXX/
│   ├── manifest.yaml
│   ├── scenarios.feature             # Generated by acceptance-test-writer
│   └── steps/                        # Generated by acceptance-test-writer (NEW)
│       └── test_prd_xxx.py
└── pytest.ini                        # [NEW] Root pytest config for acceptance-tests/
```

### Execution Modes

| Mode | Runner | Scope | When Used |
|------|--------|-------|-----------|
| `--mode=unit` | `pytest acceptance-tests/PRD-XXX/` | All scenarios | s3-orchestrator pre-impl_complete |
| `--mode=e2e` | `pytest acceptance-tests/PRD-XXX/ -v --tb=short` | All + coverage | s3-guardian blind validation |

### Tag Convention

Every scenario MUST carry:
```gherkin
@prd-EXECBDD-001 @epic-1 @feature-F1.1
Scenario: Hook file exists and is configured
```

Filtering: `pytest -k "prd-GCHAT-HOOKS-001 and epic-1"` runs only Epic 1 tests for that PRD.

### Scoring Integration

Step pass-rate feeds directly into gradient score:
```
gradient_score = (passing_scenarios / total_scenarios)
# 8/10 scenarios pass → score = 0.80
# Applies manifest.yaml weight per feature
```

This replaces LLM estimation with deterministic measurement.

---

## Epics

### Epic 1: Shared Step Definition Library
**Goal**: Create a reusable step definition library in `acceptance-tests/lib/` covering the four mechanical check categories that 80% of harness tests require.

**Features**:

**F1.1 — Filesystem Step Definitions**
- Steps: file exists/not-exists, is executable, file size > 0, directory exists
- Fixture: `repo_root` auto-detected from git root
- Pattern: `Then a file exists at {path}` → `os.path.exists(repo_root / path)`

**F1.2 — Configuration Step Definitions**
- Steps: JSON key exists, YAML key has value, settings.json hook configured
- Built-in support for `.claude/settings.json` structure
- Pattern: `Then settings.json has hook {hook_type} matching {matcher}`

**F1.3 — Content/Grep Step Definitions**
- Steps: file contains string, file does not contain string, pattern count > N
- Regex support for flexible matching
- Pattern: `And the file at {path} contains {pattern}`

**F1.4 — Subprocess Step Definitions**
- Steps: script runs without error, script output contains string, script exits with code N
- 5-second default timeout, configurable
- Pattern: `When I run {script} with args {args}, Then output contains {expected}`

**F1.5 — Shared Conftest and pytest.ini**
- `acceptance-tests/conftest.py` with `repo_root`, `settings_json`, `mcp_json` fixtures
- `acceptance-tests/pytest.ini` with marker registration and default paths
- Make lib steps auto-discoverable (`conftest.py` imports all step modules)

**Acceptance Criteria**:
- `pytest acceptance-tests/lib/ --collect-only` discovers all steps without import errors
- `pytest acceptance-tests/ -m "smoke"` runs a minimal smoke test suite
- All step functions have docstrings matching their Gherkin patterns
- Step library has zero external dependencies beyond stdlib + pytest-bdd

---

### Epic 2: Acceptance Test Writer Enhancement
**Goal**: Extend the `acceptance-test-writer` skill to generate executable step definitions (`.py`) alongside the existing Gherkin `.feature` files.

**Features**:

**F2.1 — Step Generation from "Evidence to Check" Annotations**
- Parse "Evidence to check" comments in generated `.feature` files
- Map each evidence note to a step function using the lib/ step catalog
- Generate `steps/test_prd_{id}.py` with `@given`, `@when`, `@then` decorators
- Auto-import the relevant lib step modules

**F2.2 — PRD Tag Injection**
- Inject `@prd-{ID}`, `@epic-{N}`, `@feature-{F_ID}` tags into every generated scenario
- Tags derived from feature `id` and `epic` fields in `manifest.yaml`

**F2.3 — Conftest Generation**
- Generate `acceptance-tests/PRD-XXX/steps/conftest.py` with PRD-specific fixtures
- Include `prd_id`, `prd_title` fixtures for test reporting
- Include `acceptance_manifest` fixture that loads `manifest.yaml`

**F2.4 — Skill Documentation Update**
- Update `acceptance-test-writer/SKILL.md` to document new step generation behavior
- Add section: "Step Definition Generation" with examples
- Add section: "Executing Generated Tests" with `pytest` invocation patterns

**Acceptance Criteria**:
- `Skill("acceptance-test-writer", args="--source=.taskmaster/docs/PRD-GCHAT-HOOKS-001.md")` produces `steps/test_prd_gchat_hooks.py`
- Generated step file imports from `acceptance-tests/lib/`
- Generated scenarios all have `@prd-{ID}` tags
- `pytest --collect-only acceptance-tests/PRD-GCHAT-HOOKS-001/` discovers all generated steps without errors

---

### Epic 3: Acceptance Test Runner Enhancement
**Goal**: Extend `acceptance-test-runner` to actually invoke `pytest` and return a structured JSON verdict, replacing LLM scoring with deterministic test execution.

**Features**:

**F3.1 — pytest Execution Engine**
- Replace LLM-scoring loop with `subprocess.run(["pytest", "acceptance-tests/PRD-XXX/", "--json-report", "--json-report-file=report.json"])`
- Parse `report.json` output to extract per-scenario pass/fail
- Map scenario results to feature weights from `manifest.yaml`
- Return weighted pass-rate as gradient score

**F3.2 — Structured Verdict Output**
- Emit `verdict.json` in the standard format (compatible with `verdict-schema.json`)
- Include: `score`, `status` (ACCEPT/INVESTIGATE/REJECT), `scenarios_passed`, `scenarios_total`, `feature_scores[]`
- Write to `.claude/validation/PRD-XXX-verdict.json` for audit trail

**F3.3 — Fallback Mode: LLM Scoring**
- If no step definitions exist (`steps/` directory empty), fall back to existing LLM-scoring mode
- Log: "No executable steps found, falling back to LLM scoring"
- This maintains backward compatibility for PRDs that haven't been upgraded

**F3.4 — Skill Documentation Update**
- Update `acceptance-test-runner/SKILL.md` to document pytest-first mode
- Add execution examples: `Skill("acceptance-test-runner", args="--prd=PRD-XXX --mode=unit")`

**Acceptance Criteria**:
- `Skill("acceptance-test-runner", args="--prd=PRD-GCHAT-HOOKS-001 --mode=unit")` invokes pytest and returns a verdict
- Verdict JSON validates against `verdict-schema.json`
- Score is reproducible across runs (deterministic, not LLM-estimated)
- LLM fallback activates when `steps/` directory is absent

---

### Epic 4: s3-Orchestrator Integration
**Goal**: Wire acceptance test execution into the orchestrator workflow so tests run automatically before marking `impl_complete`, preventing hollow completions.

**Features**:

**F4.1 — Pre-impl_complete Test Gate**
- Update `orchestrator.md` output style: after workers complete, before `bd update --status=impl_complete`, run `Skill("acceptance-test-runner", args="--prd={PRD_ID} --mode=unit")`
- If verdict score < `manifest.yaml` `thresholds.accept`: do NOT mark impl_complete; create a new task for the failing scenarios
- If verdict score >= threshold: proceed to `impl_complete`

**F4.2 — Worker Feedback on Test Failure**
- When tests fail pre-impl_complete, orchestrator sends test report to the responsible worker via `SendMessage`
- Message includes: failing scenario names, expected vs actual evidence
- Worker re-implements until tests pass

**F4.3 — Orchestrator Skill Documentation**
- Update `orchestrator-multiagent/SKILL.md` Phase 2 workflow with test gate step
- Add "Acceptance Test Integration" section to `WORKFLOWS.md`

**Acceptance Criteria**:
- Orchestrator workflow includes test gate step in Phase 2 Execution
- `impl_complete` is only set when acceptance score >= manifest threshold
- Failing tests generate actionable worker feedback (not just "tests failed")
- Works for PRDs with and without step definitions (falls back gracefully)

---

### Epic 5: s3-Guardian Integration
**Goal**: Enable the s3-guardian to run acceptance tests independently, replacing (or augmenting) manual LLM-scoring with deterministic pytest execution for objective blind validation.

**Features**:

**F5.1 — Guardian Test Execution**
- Update `s3-guardian/SKILL.md` Phase 2 (Validation) to invoke `acceptance-test-runner` after generating scenarios
- Guardian runs: `Skill("acceptance-test-runner", args="--prd={PRD_ID} --mode=e2e")`
- Tests run from the **implementation repository** (not harness repo) where actual code lives
- Guardian still creates `.feature` and steps in `claude-harness-setup/acceptance-tests/PRD-XXX/` (blind to operator)

**F5.2 — Step Definition Transport**
- When guardian validates an implementation repo, it copies `acceptance-tests/PRD-XXX/steps/` to the implementation repo's temp dir
- Runs pytest from there against the implementation repo's code
- Cleans up temp dir after validation
- Operator never sees the step files (they're in the harness repo)

**F5.3 — Hybrid Scoring: Executable + Gradient**
- Guardian uses executable test results as primary score
- Uses existing gradient rubric (0.0–1.0) only for scenarios WITHOUT step definitions
- Final score: weighted average of mechanical (pytest) + estimated (LLM) components
- Verdict clearly distinguishes: "7 scenarios mechanical-pass, 2 scenarios LLM-scored (0.7)"

**F5.4 — Guardian Skill Documentation**
- Update `s3-guardian/SKILL.md` with new Phase 2 workflow
- Document transport pattern (temp dir) for cross-repo testing
- Add section: "Executable vs Estimated Scenarios"

**Acceptance Criteria**:
- Guardian workflow in `SKILL.md` explicitly invokes `acceptance-test-runner`
- Verdict distinguishes mechanical vs estimated scenario scores
- Step definitions remain in harness repo (operator-blind)
- Validation evidence includes pytest output, not just LLM assessment
- Score is reproducible: running guardian twice on the same codebase returns the same score

---

## Implementation Dependencies

```
E1 (Step Library) ──────────────────────┐
                                         ▼
E2 (Writer Enhancement) ─────────────► E3 (Runner Enhancement)
                                         ▼
                    ┌────────────────────┼─────────────────────┐
                    ▼                                           ▼
               E4 (Orchestrator Integration)      E5 (Guardian Integration)
```

Epic 1 must complete before Epics 2-5. Epic 3 must complete before Epics 4 and 5.

---

## Technology Decisions

| Decision | Choice | Rationale |
|----------|--------|-----------|
| BDD Framework | `pytest-bdd` | Integrates with existing pytest, no new runner binary |
| Step Catalog | Shared `acceptance-tests/lib/` | Avoid duplication across 15+ PRD test suites |
| Report Format | `pytest-json-report` | Structured JSON, parseable without regex |
| Semantic Steps | Deferred to v2 | Mechanical steps cover 80% of current tests |
| Step Transport | Temp dir copy | Keeps steps in harness repo (guardian-blind guarantee) |

---

## Acceptance Criteria (PRD Level)

**AC1 — Runnable Tests**: `pytest acceptance-tests/PRD-GCHAT-HOOKS-001/` passes without import errors and returns a result within 30 seconds.

**AC2 — PRD Traceability**: Every generated scenario has `@prd-{ID}` and `@epic-{N}` tags. `pytest -m "prd-GCHAT-HOOKS-001"` correctly filters to that PRD's tests.

**AC3 — s3-Orchestrator Gate**: Orchestrator output style (`orchestrator.md`) includes explicit acceptance test gate in Phase 2. Gate blocks `impl_complete` when score < threshold.

**AC4 — s3-Guardian Execution**: Guardian skill (`s3-guardian/SKILL.md`) includes acceptance test runner invocation in Phase 2. Guardian verdict includes deterministic pass/fail evidence.

**AC5 — Backward Compatibility**: Existing PRDs without step definitions (`PRD-AURA-VERIFY`, `PRD-UEA-001`, etc.) still validate via LLM-scoring fallback. No regression.

**AC6 — Determinism**: Running `acceptance-test-runner` twice on the same codebase returns the same score (± 0.0 variance, not ± 0.15 like LLM scoring).

**AC7 — Living Documentation**: `.feature` files remain readable by non-technical stakeholders. Step definition code is separate and does not pollute the Gherkin spec.

---

## File Inventory (Deliverables)

| File | Epic | Type |
|------|------|------|
| `acceptance-tests/lib/conftest.py` | E1 | New |
| `acceptance-tests/lib/steps/filesystem_steps.py` | E1 | New |
| `acceptance-tests/lib/steps/config_steps.py` | E1 | New |
| `acceptance-tests/lib/steps/grep_steps.py` | E1 | New |
| `acceptance-tests/lib/steps/subprocess_steps.py` | E1 | New |
| `acceptance-tests/pytest.ini` | E1 | New |
| `.claude/skills/acceptance-test-writer/SKILL.md` | E2 | Modified |
| `acceptance-tests/PRD-GCHAT-HOOKS-001/steps/test_prd_gchat.py` | E2 | New (reference impl) |
| `acceptance-tests/PRD-GCHAT-HOOKS-001/steps/conftest.py` | E2 | New |
| `.claude/skills/acceptance-test-runner/SKILL.md` | E3 | Modified |
| `.claude/validation/verdict-schema.json` | E3 | Modified |
| `.claude/output-styles/orchestrator.md` | E4 | Modified |
| `.claude/skills/orchestrator-multiagent/SKILL.md` | E4 | Modified |
| `.claude/skills/orchestrator-multiagent/WORKFLOWS.md` | E4 | Modified |
| `.claude/skills/s3-guardian/SKILL.md` | E5 | Modified |

---

## Open Questions

1. **pytest-bdd vs behave**: This PRD recommends pytest-bdd. If the team prefers a standalone BDD runner, behave is viable but requires a separate configuration file. Recommend pytest-bdd unless existing tests use behave.

2. **Step generation quality**: Auto-generating steps from "Evidence to check" annotations requires the annotations to be consistently formatted. Some older `.feature` files may need manual step definition writing.

3. **Cross-repo testing**: The step transport pattern (temp dir) for guardian validation adds complexity. An alternative is to have guardian run tests directly against the harness repo's `acceptance-tests/` (which already mirrors the implementation repo's file paths). Evaluate based on actual directory layout.

---

## Estimated Scope

| Epic | Complexity | Estimated Sessions |
|------|-----------|-------------------|
| E1: Step Library | Medium | 1 orchestrator session (1 worker) |
| E2: Writer Enhancement | Medium | 1 orchestrator session (1 worker) |
| E3: Runner Enhancement | Medium | 1 orchestrator session (1 worker) |
| E4: Orchestrator Integration | Small | 1 orchestrator session (1 worker) |
| E5: Guardian Integration | Small | 1 orchestrator session (1 worker) |
| **Total** | | **5 orchestrator sessions** |
