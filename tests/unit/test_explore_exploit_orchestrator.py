"""Tests for the Explore-Exploit Orchestrator (Task 2.2.6).

Tests cover:
- OrchestratorConfig validation and defaults
- IterationSnapshot model
- OrchestrationResult model and serialization
- ExploreExploitOrchestrator initialization
- Full orchestration loop with mocked components
- Exploitation phase
- Exploration phase
- Diversity sampling phase
- LLM filtering phase
- Convergence and early stopping
- Edge cases (empty results, errors, no LLM)
- Package-level imports
"""

from __future__ import annotations

import json
from typing import Any
from unittest.mock import MagicMock, patch

import pytest
from pydantic import ValidationError

from cobuilder.repomap.llm.models import ModelTier
from cobuilder.repomap.ontology.models import FeatureNode, FeaturePath, OntologyStats
from cobuilder.repomap.selection.convergence import ConvergenceConfig
from cobuilder.repomap.selection.diversity_sampler import DiversityConfig, DiversityMetrics
from cobuilder.repomap.selection.exploitation import ExploitationConfig
from cobuilder.repomap.selection.exploration import (
    CoverageTracker,
    ExplorationConfig,
    ExplorationResult,
)
from cobuilder.repomap.selection.llm_filter import FilterResult, LLMFilterConfig
from cobuilder.repomap.selection.orchestrator import (
    ExploreExploitOrchestrator,
    IterationSnapshot,
    OrchestrationResult,
    OrchestratorConfig,
)


# ---------------------------------------------------------------------------
# Fixtures
# ---------------------------------------------------------------------------


def _make_feature(
    fid: str,
    name: str,
    level: int = 2,
    score: float = 0.8,
    embedding: list[float] | None = None,
    parent_id: str | None = None,
    tags: list[str] | None = None,
) -> FeaturePath:
    """Create a FeaturePath with a single-node path for testing."""
    node = FeatureNode(
        id=fid,
        name=name,
        level=level,
        parent_id=parent_id,
        tags=tags or [],
        embedding=embedding,
    )
    return FeaturePath(nodes=[node], score=score)


def _make_store_mock() -> MagicMock:
    """Create a mock OntologyBackend."""
    store = MagicMock()
    store.get_statistics.return_value = OntologyStats(
        total_nodes=100,
        total_levels=4,
        avg_children=3.5,
        max_depth=4,
        root_count=10,
        leaf_count=60,
        nodes_with_embeddings=80,
    )
    store.search.return_value = []
    store.search_with_filters.return_value = []
    return store


def _make_llm_mock() -> MagicMock:
    """Create a mock LLMGateway."""
    llm = MagicMock()
    llm.select_model.return_value = "test-model"
    llm.complete.return_value = '{"queries": ["test query 1", "test query 2"]}'
    return llm


# ---------------------------------------------------------------------------
# OrchestratorConfig tests
# ---------------------------------------------------------------------------


class TestOrchestratorConfig:
    """Tests for OrchestratorConfig model."""

    def test_defaults(self) -> None:
        cfg = OrchestratorConfig()
        assert cfg.max_iterations == 30
        assert cfg.exploit_top_k == 50
        assert cfg.diversity_threshold == 0.85
        assert cfg.min_avg_distance == 0.5
        assert cfg.features_per_iteration == 5
        assert cfg.filter_interval == 5
        assert cfg.enable_exploration is True
        assert cfg.enable_filtering is True
        assert cfg.enable_convergence is True

    def test_custom(self) -> None:
        cfg = OrchestratorConfig(
            max_iterations=10,
            exploit_top_k=100,
            diversity_threshold=0.7,
            features_per_iteration=10,
            filter_interval=3,
        )
        assert cfg.max_iterations == 10
        assert cfg.exploit_top_k == 100
        assert cfg.diversity_threshold == 0.7

    def test_invalid_iterations(self) -> None:
        with pytest.raises(ValidationError):
            OrchestratorConfig(max_iterations=0)

    def test_invalid_diversity_threshold(self) -> None:
        with pytest.raises(ValidationError):
            OrchestratorConfig(diversity_threshold=0.0)

    def test_sub_configs(self) -> None:
        cfg = OrchestratorConfig(
            exploitation_config=ExploitationConfig(max_augmented_queries=3),
            convergence_config=ConvergenceConfig(max_iterations=15),
        )
        assert cfg.exploitation_config.max_augmented_queries == 3
        assert cfg.convergence_config.max_iterations == 15


# ---------------------------------------------------------------------------
# IterationSnapshot tests
# ---------------------------------------------------------------------------


class TestIterationSnapshot:
    """Tests for IterationSnapshot model."""

    def test_basic(self) -> None:
        snap = IterationSnapshot(
            iteration=1,
            exploit_count=50,
            explore_count=20,
            candidates_merged=65,
            selected_count=5,
            total_selected=5,
            coverage=0.15,
            duration_ms=120.5,
        )
        assert snap.iteration == 1
        assert snap.exploit_count == 50
        assert snap.selected_count == 5

    def test_frozen(self) -> None:
        snap = IterationSnapshot(iteration=1)
        with pytest.raises(ValidationError):
            snap.iteration = 2  # type: ignore[misc]


# ---------------------------------------------------------------------------
# OrchestrationResult tests
# ---------------------------------------------------------------------------


class TestOrchestrationResult:
    """Tests for OrchestrationResult model."""

    def test_empty(self) -> None:
        result = OrchestrationResult()
        assert result.count == 0
        assert result.iterations_run == 0
        assert result.stop_reason == "max_iterations"

    def test_properties(self) -> None:
        paths = [
            _make_feature("f1", "Feature 1", score=0.9),
            _make_feature("f2", "Feature 2", score=0.8),
        ]
        result = OrchestrationResult(
            selected=paths,
            iterations_run=5,
            stop_reason="converged",
        )
        assert result.count == 2
        assert result.stop_reason == "converged"

    def test_to_json(self) -> None:
        paths = [
            _make_feature("f1", "Feature 1", score=0.9, tags=["backend"]),
            _make_feature("f2", "Feature 2", score=0.8, tags=["frontend"]),
        ]
        result = OrchestrationResult(
            selected=paths,
            iterations_run=3,
            stop_reason="converged",
        )
        json_str = result.to_json()
        data = json.loads(json_str)
        assert data["count"] == 2
        assert data["iterations_run"] == 3
        assert data["stop_reason"] == "converged"
        assert len(data["features"]) == 2
        assert data["features"][0]["id"] == "f1"
        assert data["features"][0]["name"] == "Feature 1"
        assert data["features"][0]["tags"] == ["backend"]

    def test_frozen(self) -> None:
        result = OrchestrationResult()
        with pytest.raises(ValidationError):
            result.iterations_run = 5  # type: ignore[misc]


# ---------------------------------------------------------------------------
# Orchestrator initialization tests
# ---------------------------------------------------------------------------


class TestOrchestratorInit:
    """Tests for ExploreExploitOrchestrator initialization."""

    def test_default_config(self) -> None:
        store = _make_store_mock()
        orch = ExploreExploitOrchestrator(store=store)
        assert orch.config.max_iterations == 30
        assert orch.store is store

    def test_custom_config(self) -> None:
        store = _make_store_mock()
        cfg = OrchestratorConfig(max_iterations=10)
        orch = ExploreExploitOrchestrator(store=store, config=cfg)
        assert orch.config.max_iterations == 10

    def test_with_llm(self) -> None:
        store = _make_store_mock()
        llm = _make_llm_mock()
        orch = ExploreExploitOrchestrator(store=store, llm_gateway=llm)
        assert orch.config is not None

    def test_empty_spec_raises(self) -> None:
        store = _make_store_mock()
        orch = ExploreExploitOrchestrator(store=store)
        with pytest.raises(ValueError, match="spec_description"):
            orch.run(spec_description="")

    def test_whitespace_spec_raises(self) -> None:
        store = _make_store_mock()
        orch = ExploreExploitOrchestrator(store=store)
        with pytest.raises(ValueError, match="spec_description"):
            orch.run(spec_description="   ")


# ---------------------------------------------------------------------------
# Full orchestration loop tests
# ---------------------------------------------------------------------------


class TestOrchestrationLoop:
    """Tests for the full orchestration loop."""

    def test_basic_run_no_results(self) -> None:
        """Run with a store that returns no results."""
        store = _make_store_mock()
        cfg = OrchestratorConfig(
            max_iterations=3,
            enable_exploration=False,
            enable_filtering=False,
            enable_convergence=False,
        )
        orch = ExploreExploitOrchestrator(store=store, config=cfg)
        result = orch.run(spec_description="Test specification")

        assert result.iterations_run == 3
        assert result.count == 0
        assert result.stop_reason == "max_iterations"
        assert len(result.iteration_history) == 3

    def test_run_with_features(self) -> None:
        """Run with a store returning features."""
        store = _make_store_mock()
        features = [
            _make_feature(f"f{i}", f"Feature {i}", score=0.9 - i * 0.1,
                          embedding=[float(i)] * 10)
            for i in range(5)
        ]
        store.search.return_value = features
        store.search_with_filters.return_value = features

        cfg = OrchestratorConfig(
            max_iterations=3,
            features_per_iteration=3,
            enable_exploration=False,
            enable_filtering=False,
            enable_convergence=False,
        )
        orch = ExploreExploitOrchestrator(store=store, config=cfg)
        result = orch.run(spec_description="Build a web application")

        assert result.iterations_run == 3
        assert result.count > 0
        assert result.diversity_metrics is not None

    def test_convergence_stops_early(self) -> None:
        """Convergence monitor triggers early stop at plateau."""
        store = _make_store_mock()

        cfg = OrchestratorConfig(
            max_iterations=30,
            enable_exploration=False,
            enable_filtering=False,
            enable_convergence=True,
            convergence_config=ConvergenceConfig(
                plateau_window=3,
                plateau_threshold=0.01,
                target_coverage=0.95,
                max_iterations=30,
            ),
        )
        orch = ExploreExploitOrchestrator(store=store, config=cfg)
        result = orch.run(spec_description="Test convergence")

        # With no features being selected (empty store), coverage stays at 0
        # After plateau_window iterations, plateau should trigger
        assert result.iterations_run < 30
        assert result.stop_reason in ("plateau", "max_iterations")

    def test_run_with_languages_and_frameworks(self) -> None:
        """Run with optional spec parameters."""
        store = _make_store_mock()
        cfg = OrchestratorConfig(
            max_iterations=2,
            enable_exploration=False,
            enable_filtering=False,
            enable_convergence=False,
        )
        orch = ExploreExploitOrchestrator(store=store, config=cfg)
        result = orch.run(
            spec_description="Chat app",
            spec_languages=["Python", "TypeScript"],
            spec_frameworks=["FastAPI", "React"],
        )
        assert result.iterations_run == 2
        assert result.metadata["spec_languages"] == ["Python", "TypeScript"]
        assert result.metadata["spec_frameworks"] == ["FastAPI", "React"]


# ---------------------------------------------------------------------------
# Phase-specific tests
# ---------------------------------------------------------------------------


class TestExploitationPhase:
    """Tests for the exploitation phase."""

    def test_retrieves_features(self) -> None:
        store = _make_store_mock()
        features = [_make_feature("f1", "Auth", score=0.9)]
        store.search.return_value = features
        store.search_with_filters.return_value = features

        cfg = OrchestratorConfig(
            max_iterations=1,
            enable_exploration=False,
            enable_filtering=False,
            enable_convergence=False,
        )
        orch = ExploreExploitOrchestrator(store=store, config=cfg)
        result = orch.run(spec_description="Authentication system")

        assert result.iteration_history[0].exploit_count > 0

    def test_handles_retrieval_error(self) -> None:
        store = _make_store_mock()
        store.search.side_effect = RuntimeError("Connection lost")
        store.search_with_filters.side_effect = RuntimeError("Connection lost")

        cfg = OrchestratorConfig(
            max_iterations=1,
            enable_exploration=False,
            enable_filtering=False,
            enable_convergence=False,
        )
        orch = ExploreExploitOrchestrator(store=store, config=cfg)
        # Should not raise; logs warning and continues
        result = orch.run(spec_description="Test error handling")
        assert result.iterations_run == 1
        assert result.count == 0


class TestExplorationPhase:
    """Tests for the exploration phase."""

    def test_exploration_disabled(self) -> None:
        store = _make_store_mock()
        cfg = OrchestratorConfig(
            max_iterations=1,
            enable_exploration=False,
            enable_filtering=False,
            enable_convergence=False,
        )
        orch = ExploreExploitOrchestrator(store=store, config=cfg)
        result = orch.run(spec_description="Test no exploration")

        assert result.iteration_history[0].explore_count == 0

    def test_exploration_enabled(self) -> None:
        store = _make_store_mock()
        cfg = OrchestratorConfig(
            max_iterations=1,
            enable_exploration=True,
            enable_filtering=False,
            enable_convergence=False,
        )
        orch = ExploreExploitOrchestrator(store=store, config=cfg)
        result = orch.run(spec_description="Test with exploration")

        # Even if 0 exploration results, the phase should have been attempted
        assert result.iterations_run == 1


class TestDiversityPhase:
    """Tests for the diversity sampling phase."""

    def test_diverse_features_selected(self) -> None:
        """Features with different embeddings should be selected."""
        store = _make_store_mock()

        # Create features with very different embeddings
        features = [
            _make_feature("f1", "Auth", score=0.9,
                          embedding=[1.0, 0.0, 0.0, 0.0, 0.0]),
            _make_feature("f2", "Database", score=0.85,
                          embedding=[0.0, 1.0, 0.0, 0.0, 0.0]),
            _make_feature("f3", "API", score=0.8,
                          embedding=[0.0, 0.0, 1.0, 0.0, 0.0]),
        ]
        store.search.return_value = features
        store.search_with_filters.return_value = features

        cfg = OrchestratorConfig(
            max_iterations=1,
            features_per_iteration=10,
            diversity_threshold=0.85,
            enable_exploration=False,
            enable_filtering=False,
            enable_convergence=False,
        )
        orch = ExploreExploitOrchestrator(store=store, config=cfg)
        result = orch.run(spec_description="Full stack app")

        # All 3 should be selected (orthogonal embeddings)
        assert result.count == 3

    def test_similar_features_rejected(self) -> None:
        """Features with very similar embeddings should be rejected."""
        store = _make_store_mock()

        # Create features with nearly identical embeddings
        features = [
            _make_feature("f1", "Auth Login", score=0.9,
                          embedding=[1.0, 0.0, 0.0]),
            _make_feature("f2", "Auth Register", score=0.85,
                          embedding=[0.99, 0.01, 0.0]),
            _make_feature("f3", "Auth SSO", score=0.8,
                          embedding=[0.98, 0.02, 0.0]),
        ]
        store.search.return_value = features
        store.search_with_filters.return_value = features

        cfg = OrchestratorConfig(
            max_iterations=1,
            features_per_iteration=10,
            diversity_threshold=0.85,
            enable_exploration=False,
            enable_filtering=False,
            enable_convergence=False,
        )
        orch = ExploreExploitOrchestrator(store=store, config=cfg)
        result = orch.run(spec_description="Auth system")

        # Only first should be selected; others are too similar
        assert result.count == 1


class TestFilteringPhase:
    """Tests for the LLM filtering phase."""

    def test_filtering_at_interval(self) -> None:
        """LLM filter runs at the configured interval."""
        store = _make_store_mock()
        llm = _make_llm_mock()

        features = [
            _make_feature("f1", "Auth", score=0.9,
                          embedding=[1.0, 0.0, 0.0, 0.0, 0.0]),
        ]
        store.search.return_value = features
        store.search_with_filters.return_value = features

        # LLM filter should keep all features
        llm.complete.return_value = json.dumps({
            "decisions": [
                {"feature_id": "f1", "action": "keep", "confidence": 0.9,
                 "reason": "Relevant"}
            ]
        })

        cfg = OrchestratorConfig(
            max_iterations=5,
            filter_interval=5,
            enable_exploration=False,
            enable_filtering=True,
            enable_convergence=False,
        )
        orch = ExploreExploitOrchestrator(
            store=store, llm_gateway=llm, config=cfg
        )
        result = orch.run(spec_description="Auth system")

        # Iteration 5 should have filtered=True
        assert result.iteration_history[4].filtered is True

    def test_filtering_disabled(self) -> None:
        store = _make_store_mock()
        cfg = OrchestratorConfig(
            max_iterations=5,
            enable_filtering=False,
            enable_exploration=False,
            enable_convergence=False,
        )
        orch = ExploreExploitOrchestrator(store=store, config=cfg)
        result = orch.run(spec_description="Test no filter")

        assert all(
            snap.filtered is False
            for snap in result.iteration_history
        )


# ---------------------------------------------------------------------------
# Merge and de-duplication tests
# ---------------------------------------------------------------------------


class TestMergeCandidates:
    """Tests for candidate merging and de-duplication."""

    def test_dedup_by_leaf_id(self) -> None:
        store = _make_store_mock()
        orch = ExploreExploitOrchestrator(store=store)

        exploit = [
            _make_feature("f1", "Auth", score=0.9),
            _make_feature("f2", "DB", score=0.8),
        ]
        explore = [
            _make_feature("f1", "Auth", score=0.7),  # Duplicate
            _make_feature("f3", "API", score=0.6),
        ]

        merged = orch._merge_candidates(exploit, explore, set())
        ids = [p.leaf.id for p in merged]
        assert len(ids) == 3
        assert ids == ["f1", "f2", "f3"]  # Sorted by score
        # f1 should keep the higher score (0.9)
        assert merged[0].score == 0.9

    def test_excludes_already_selected(self) -> None:
        store = _make_store_mock()
        orch = ExploreExploitOrchestrator(store=store)

        exploit = [
            _make_feature("f1", "Auth", score=0.9),
            _make_feature("f2", "DB", score=0.8),
        ]

        merged = orch._merge_candidates(exploit, [], {"f1"})
        assert len(merged) == 1
        assert merged[0].leaf.id == "f2"


# ---------------------------------------------------------------------------
# Convergence tests
# ---------------------------------------------------------------------------


class TestConvergence:
    """Tests for convergence monitoring integration."""

    def test_convergence_summary_in_result(self) -> None:
        store = _make_store_mock()
        cfg = OrchestratorConfig(
            max_iterations=3,
            enable_convergence=True,
            enable_exploration=False,
            enable_filtering=False,
        )
        orch = ExploreExploitOrchestrator(store=store, config=cfg)
        result = orch.run(spec_description="Test")

        assert result.convergence_summary is not None
        assert result.convergence_summary.total_iterations == 3

    def test_target_coverage_stops(self) -> None:
        """If target coverage is reached, stop early."""
        store = _make_store_mock()

        # Return features with distinct IDs each iteration
        call_count = 0

        def make_features(*args: Any, **kwargs: Any) -> list[FeaturePath]:
            nonlocal call_count
            call_count += 1
            return [
                _make_feature(
                    f"f{call_count}_{j}",
                    f"Feature {call_count}_{j}",
                    score=0.9,
                    embedding=[float(call_count * 10 + j)] * 5,
                )
                for j in range(5)
            ]

        store.search.side_effect = make_features
        store.search_with_filters.side_effect = make_features

        cfg = OrchestratorConfig(
            max_iterations=30,
            enable_convergence=True,
            enable_exploration=False,
            enable_filtering=False,
            convergence_config=ConvergenceConfig(
                target_coverage=0.01,  # Very low target = converge fast
            ),
        )
        orch = ExploreExploitOrchestrator(store=store, config=cfg)

        # Register a small set of nodes so coverage can reach 100%
        for i in range(5):
            orch.coverage.register_node(
                f"f1_{i}", f"Feature 1_{i}", level=2
            )

        result = orch.run(spec_description="Quick convergence test")

        # Should stop early due to convergence
        assert result.iterations_run < 30


# ---------------------------------------------------------------------------
# Edge case tests
# ---------------------------------------------------------------------------


class TestEdgeCases:
    """Tests for edge cases and error handling."""

    def test_no_llm_gateway(self) -> None:
        """Orchestrator works without an LLM gateway."""
        store = _make_store_mock()
        features = [
            _make_feature("f1", "Feature 1", score=0.9, embedding=[1.0, 0.0]),
        ]
        store.search.return_value = features
        store.search_with_filters.return_value = features

        cfg = OrchestratorConfig(
            max_iterations=2,
            enable_exploration=False,
            enable_filtering=True,  # Should be no-op without LLM
            enable_convergence=False,
        )
        orch = ExploreExploitOrchestrator(store=store, config=cfg)
        result = orch.run(spec_description="No LLM test")

        assert result.iterations_run == 2
        assert result.count >= 1

    def test_store_statistics_error(self) -> None:
        """Gracefully handles store statistics failure."""
        store = _make_store_mock()
        store.get_statistics.side_effect = RuntimeError("Not available")

        cfg = OrchestratorConfig(
            max_iterations=1,
            enable_exploration=False,
            enable_filtering=False,
            enable_convergence=False,
        )
        orch = ExploreExploitOrchestrator(store=store, config=cfg)
        result = orch.run(spec_description="Stats error test")
        assert result.iterations_run == 1

    def test_single_iteration(self) -> None:
        store = _make_store_mock()
        cfg = OrchestratorConfig(
            max_iterations=1,
            enable_exploration=False,
            enable_filtering=False,
            enable_convergence=False,
        )
        orch = ExploreExploitOrchestrator(store=store, config=cfg)
        result = orch.run(spec_description="Single iteration")
        assert result.iterations_run == 1


# ---------------------------------------------------------------------------
# Import tests
# ---------------------------------------------------------------------------


class TestImports:
    """Tests for package-level imports."""

    def test_import_from_package(self) -> None:
        from cobuilder.repomap.selection import (
            ExploreExploitOrchestrator,
            IterationSnapshot,
            OrchestrationResult,
            OrchestratorConfig,
        )

        assert ExploreExploitOrchestrator is not None
        assert OrchestratorConfig is not None
        assert OrchestrationResult is not None
        assert IterationSnapshot is not None

    def test_import_from_module(self) -> None:
        from cobuilder.repomap.selection.orchestrator import (
            ExploreExploitOrchestrator,
            IterationSnapshot,
            OrchestrationResult,
            OrchestratorConfig,
        )

        assert ExploreExploitOrchestrator is not None
